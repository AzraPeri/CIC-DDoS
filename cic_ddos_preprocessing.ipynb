{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4059918,"sourceType":"datasetVersion","datasetId":2398189}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# AdÄ±m 1: Kurulum, veri yÃ¼kleme, temizlik (Kaggle iÃ§in)\nimport os, glob, warnings, numpy as np, pandas as pd\nwarnings.filterwarnings(\"ignore\")\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_DIR = \"/kaggle/input/cicddos2019\"\nTARGET   = \"__label__\"\n\ndef infer_label(fname):  # Syn-training.parquet -> Syn\n    return os.path.basename(fname).split(\"-\")[0]\ndef infer_split(fname):  # train/test bilgisi\n    b = os.path.basename(fname).lower()\n    return \"train\" if \"train\" in b else (\"test\" if \"test\" in b else \"unknown\")\n\n# 1.1 Parquetleri birleÅŸtir\nframes=[]\nfor f in sorted(glob.glob(os.path.join(DATA_DIR,\"*.parquet\"))):\n    df = pd.read_parquet(f)\n    df[\"__split__\"] = infer_split(f)\n    df[TARGET]      = infer_label(f)\n    frames.append(df)\nraw = pd.concat(frames, ignore_index=True)\n\n# 1.2 Temizlik: inf->NaN, kategorik/sabit kolonlarÄ± at\nraw = raw.replace([np.inf,-np.inf], np.nan)\nMETA = [TARGET,\"__split__\"]\nfeatures = [c for c in raw.columns if c not in META]\nnum_cols = [c for c in features if pd.api.types.is_numeric_dtype(raw[c])]\ndf = raw.drop(columns=[c for c in features if c not in num_cols]).copy()\nconstant = df[num_cols].nunique(dropna=False)\nconst_cols = constant[constant<=1].index.tolist()\nif const_cols:\n    df.drop(columns=const_cols, inplace=True)\n\n# 1.3 Train/Test ayÄ±r\ntrain_df = df[df[\"__split__\"]==\"train\"].copy()\ntest_df  = df[df[\"__split__\"]==\"test\"].copy()\nX_train, y_train = train_df.drop(columns=META), train_df[TARGET].astype(str)\nX_test,  y_test  = test_df.drop(columns=META),  test_df[TARGET].astype(str)\n\nprint(\"YÃ¼klendi âœ“\",\n      \"\\nTrain:\", X_train.shape, \"| Test:\", X_test.shape,\n      \"\\nTrain sÄ±nÄ±flarÄ±:\", sorted(y_train.unique()),\n      \"\\nTest  sÄ±nÄ±flarÄ±:\", sorted(y_test.unique()))","metadata":{"_uuid":"eee84e43-0835-4c02-bf96-e46f9b4303bf","_cell_guid":"aed33c85-0690-47cf-a68b-b7eb0094027e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-14T12:04:55.616307Z","iopub.execute_input":"2025-11-14T12:04:55.616976Z","iopub.status.idle":"2025-11-14T12:05:01.164029Z","shell.execute_reply.started":"2025-11-14T12:04:55.616942Z","shell.execute_reply":"2025-11-14T12:05:01.162904Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"YÃ¼klendi âœ“ \nTrain: (125170, 65) | Test: (306201, 65) \nTrain sÄ±nÄ±flarÄ±: ['LDAP', 'MSSQL', 'NetBIOS', 'Portmap', 'Syn', 'UDP', 'UDPLag'] \nTest  sÄ±nÄ±flarÄ±: ['DNS', 'LDAP', 'MSSQL', 'NTP', 'NetBIOS', 'SNMP', 'Syn', 'TFTP', 'UDP', 'UDPLag']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# AdÄ±m 2: Ortak sÄ±nÄ±flar (closed-set)\ncommon = sorted(set(y_train.unique()).intersection(set(y_test.unique())))\ntrain_mask = y_train.isin(common)\ntest_mask  = y_test.isin(common)\n\nX_tr, y_tr = X_train[train_mask].copy(), y_train[train_mask].copy()\nX_te, y_te = X_test[test_mask].copy(),  y_test[test_mask].copy()\n\nprint(\"Ortak sÄ±nÄ±flar:\", common)\nprint(\"Yeni Train/Test:\", X_tr.shape, X_te.shape)","metadata":{"_uuid":"69925753-166a-4e54-a3c5-97ecbbaf48da","_cell_guid":"bc64fac1-1586-4f48-9de1-c38c738f2b80","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-14T12:05:01.165090Z","iopub.execute_input":"2025-11-14T12:05:01.165633Z","iopub.status.idle":"2025-11-14T12:05:01.271960Z","shell.execute_reply.started":"2025-11-14T12:05:01.165601Z","shell.execute_reply":"2025-11-14T12:05:01.270846Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Ortak sÄ±nÄ±flar: ['LDAP', 'MSSQL', 'NetBIOS', 'Syn', 'UDP', 'UDPLag']\nYeni Train/Test: (120065, 65) (38973, 65)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# AdÄ±m 3: Manuel undersampling (imblearn yok)\nfrom collections import Counter\ntmp = X_tr.copy(); tmp[TARGET]=y_tr.values\nmin_count = tmp[TARGET].value_counts().min()\nbalanced = (tmp.groupby(TARGET, group_keys=False)\n              .apply(lambda x: x.sample(min_count, random_state=RANDOM_STATE)))\ny_tr_bal = balanced[TARGET].astype(str)\nX_tr_bal = balanced.drop(columns=TARGET)\n\nprint(\"Dengeleme Ã¶nce:\", Counter(y_tr))\nprint(\"Dengeleme sonra:\", Counter(y_tr_bal))\nprint(\"Yeni eÄŸitim boyutu:\", X_tr_bal.shape)","metadata":{"_uuid":"2af037f4-75e0-4ff7-b1e6-b49d70344525","_cell_guid":"ae2497c3-b836-4379-b426-a40bd9c1963c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-14T12:05:01.274281Z","iopub.execute_input":"2025-11-14T12:05:01.275010Z","iopub.status.idle":"2025-11-14T12:05:01.380027Z","shell.execute_reply.started":"2025-11-14T12:05:01.274977Z","shell.execute_reply":"2025-11-14T12:05:01.378942Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Dengeleme Ã¶nce: Counter({'Syn': 70336, 'UDP': 17770, 'UDPLag': 12639, 'MSSQL': 10974, 'LDAP': 6715, 'NetBIOS': 1631})\nDengeleme sonra: Counter({'LDAP': 1631, 'MSSQL': 1631, 'NetBIOS': 1631, 'Syn': 1631, 'UDP': 1631, 'UDPLag': 1631})\nYeni eÄŸitim boyutu: (9786, 65)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- AdÄ±m 4 (final patch): mutual_info_score ile manuel Ã¶zellik seÃ§imi + LGBM ---\nimport numpy as np, pandas as pd\nfrom sklearn.metrics import mutual_info_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom lightgbm import LGBMClassifier\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\ndef mi_scores(X, y):\n    \"\"\"Her sÃ¼tun iÃ§in mutual information puanÄ± hesapla (saf sklearn.metrics)\"\"\"\n    scores = {}\n    y_enc = pd.factorize(y)[0]\n    for col in X.columns:\n        # SÃ¼rekli deÄŸiÅŸkenleri kategoriye bÃ¶lmek iÃ§in kÃ¼Ã§Ã¼k quantile binning\n        x_bin = pd.qcut(X[col].rank(method=\"first\"), q=10, duplicates=\"drop\")\n        x_enc = pd.factorize(x_bin)[0]\n        mi = mutual_info_score(x_enc, y_enc)\n        scores[col] = mi\n    return pd.Series(scores).sort_values(ascending=False)\n\n# MI puanlarÄ±nÄ± hesapla\nmi = mi_scores(X_tr_bal, y_tr_bal)\nprint(\"En yÃ¼ksek bilgiye sahip 10 Ã¶zellik:\")\nprint(mi.head(10))\n\n# k deÄŸerleri listesi\nk_list = [15, 25, 35, 50]\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n\nbest_k, best_cv, selected_cols = None, -1.0, None\n\nfor k in k_list:\n    top_cols = mi.head(k).index\n    X_sel = X_tr_bal[top_cols]\n    clf = LGBMClassifier(\n        n_estimators=200, learning_rate=0.1,\n        num_leaves=31, subsample=0.8, colsample_bytree=0.8,\n        random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1\n    )\n    acc = cross_val_score(clf, X_sel, y_tr_bal, cv=cv, scoring=\"accuracy\").mean()\n    print(f\"k={k:2d} | CV Acc={acc:.4f}\")\n    if acc > best_cv:\n        best_cv, best_k, selected_cols = acc, k, top_cols\n\nprint(f\"\\nâœ… En iyi k={best_k} (CV Acc={best_cv:.4f})\")\nprint(\"SeÃ§ilen ilk 10 Ã¶zellik:\", list(selected_cols[:10]))\n\n# seÃ§ilen kolonlarla train/test setleri\nX_tr_sel = X_tr_bal[selected_cols].copy()\nX_te_sel = X_te[selected_cols].copy()","metadata":{"_uuid":"d11acdb7-ce44-4f85-aecc-794399d4058b","_cell_guid":"18071190-9fb2-4e4a-a5ee-d2f268f3c2fe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-14T12:05:01.380975Z","iopub.execute_input":"2025-11-14T12:05:01.381282Z","iopub.status.idle":"2025-11-14T12:05:44.124626Z","shell.execute_reply.started":"2025-11-14T12:05:01.381257Z","shell.execute_reply":"2025-11-14T12:05:44.123723Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"En yÃ¼ksek bilgiye sahip 10 Ã¶zellik:\nSYN Flag Count           1.524883\nCWE Flag Count           1.405789\nRST Flag Count           1.385206\nFwd PSH Flags            1.385206\nBwd Packet Length Std    1.377581\nACK Flag Count           1.339930\nActive Min               1.300542\nProtocol                 1.294937\nActive Std               1.292313\nIdle Std                 1.291538\ndtype: float64\nk=15 | CV Acc=0.3732\nk=25 | CV Acc=0.5504\nk=35 | CV Acc=0.6403\nk=50 | CV Acc=0.6605\n\nâœ… En iyi k=50 (CV Acc=0.6605)\nSeÃ§ilen ilk 10 Ã¶zellik: ['SYN Flag Count', 'CWE Flag Count', 'RST Flag Count', 'Fwd PSH Flags', 'Bwd Packet Length Std', 'ACK Flag Count', 'Active Min', 'Protocol', 'Active Std', 'Idle Std']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# === TAM VERÄ° HAZIRLIK (CIC-DDoS + dengeleme + Ã¶zellik seÃ§imi) ===\nimport os, glob, warnings, numpy as np, pandas as pd\nfrom collections import Counter\nfrom sklearn.metrics import mutual_info_score\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nDATA_DIR = \"/kaggle/input/cicddos2019\"\nTARGET = \"__label__\"\n\n# --- 1. Veri yÃ¼kleme ---\ndef infer_label(fname):  # Syn-training.parquet -> Syn\n    return os.path.basename(fname).split(\"-\")[0]\n\ndef infer_split(fname):  # train/test bilgisi\n    b = os.path.basename(fname).lower()\n    return \"train\" if \"train\" in b else (\"test\" if \"test\" in b else \"unknown\")\n\nframes = []\nfor f in sorted(glob.glob(os.path.join(DATA_DIR, \"*.parquet\"))):\n    df = pd.read_parquet(f)\n    df[\"__split__\"] = infer_split(f)\n    df[TARGET] = infer_label(f)\n    frames.append(df)\n\nraw = pd.concat(frames, ignore_index=True)\nraw = raw.replace([np.inf, -np.inf], np.nan)\n\n# --- 2. SÃ¼tun temizliÄŸi ---\nMETA = [TARGET, \"__split__\"]\nfeatures = [c for c in raw.columns if c not in META]\nnum_cols = [c for c in features if pd.api.types.is_numeric_dtype(raw[c])]\ndf = raw.drop(columns=[c for c in features if c not in num_cols]).copy()\n\nconstant = df[num_cols].nunique(dropna=False)\nconst_cols = constant[constant <= 1].index.tolist()\nif const_cols:\n    df.drop(columns=const_cols, inplace=True)\n\n# --- 3. Train/Test ayÄ±r ---\ntrain_df = df[df[\"__split__\"] == \"train\"].copy()\ntest_df = df[df[\"__split__\"] == \"test\"].copy()\nX_train, y_train = train_df.drop(columns=META), train_df[TARGET].astype(str)\nX_test, y_test = test_df.drop(columns=META), test_df[TARGET].astype(str)\n\nprint(\"YÃ¼klendi âœ“\")\nprint(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\n\n# --- 4. Closed-set filtreleme ---\ncommon = sorted(set(y_train.unique()).intersection(set(y_test.unique())))\ntrain_mask = y_train.isin(common)\ntest_mask = y_test.isin(common)\nX_tr = X_train[train_mask].copy()\ny_tr = y_train[train_mask].copy()\nX_te = X_test[test_mask].copy()\ny_te = y_test[test_mask].copy()\nprint(\"Ortak sÄ±nÄ±flar:\", common)\n\n# --- 5. Manuel dengeleme (undersampling) ---\ntmp = X_tr.copy()\ntmp[\"__y__\"] = y_tr.values\nmin_count = tmp[\"__y__\"].value_counts().min()\nbalanced = tmp.groupby(\"__y__\", group_keys=False).apply(\n    lambda x: x.sample(min_count, random_state=RANDOM_STATE)\n)\ny_tr_bal = balanced[\"__y__\"].astype(str)\nX_tr_bal = balanced.drop(columns=\"__y__\")\nprint(\"Denge sonrasÄ± boyut:\", X_tr_bal.shape)\n\n# --- 6. Ã–zellik seÃ§imi (Mutual Information ile) ---\ndef mi_scores(X, y):\n    scores = {}\n    y_enc = pd.factorize(y)[0]\n    for col in X.columns:\n        x_bin = pd.qcut(X[col].rank(method=\"first\"), q=10, duplicates=\"drop\")\n        x_enc = pd.factorize(x_bin)[0]\n        scores[col] = mutual_info_score(x_enc, y_enc)\n    return pd.Series(scores).sort_values(ascending=False)\n\nmi = mi_scores(X_tr_bal, y_tr_bal)\nselected_cols = mi.head(25).index\nX_tr_sel = X_tr_bal[selected_cols].copy()\nX_te_sel = X_te[selected_cols].copy()\n\nprint(\"\\nâœ… Veri tamamen hazÄ±r!\")\nprint(\"EÄŸitim:\", X_tr_sel.shape, \"| Test:\", X_te_sel.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T12:05:44.125919Z","iopub.execute_input":"2025-11-14T12:05:44.126624Z","iopub.status.idle":"2025-11-14T12:05:46.317293Z","shell.execute_reply.started":"2025-11-14T12:05:44.126586Z","shell.execute_reply":"2025-11-14T12:05:46.316454Z"}},"outputs":[{"name":"stdout","text":"YÃ¼klendi âœ“\nTrain: (125170, 65) | Test: (306201, 65)\nOrtak sÄ±nÄ±flar: ['LDAP', 'MSSQL', 'NetBIOS', 'Syn', 'UDP', 'UDPLag']\nDenge sonrasÄ± boyut: (9786, 65)\n\nâœ… Veri tamamen hazÄ±r!\nEÄŸitim: (9786, 25) | Test: (38973, 25)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# === GeliÅŸtirilmiÅŸ MBO: Acc-F1 fitness + k (Ã¶zellik sayÄ±sÄ±) optimizasyonu ===\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, mutual_info_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.utils import check_random_state\n\nRANDOM_STATE = 42\nrng = check_random_state(RANDOM_STATE)\n\n# --- MI puanlarÄ±nÄ± hesapla (Ã¶zellik sayÄ±sÄ± k'yÄ± MBO seÃ§ecek) ---\ndef mi_scores(X, y, q=10):\n    scores = {}\n    y_enc = pd.factorize(y)[0]\n    for col in X.columns:\n        x_bin = pd.qcut(X[col].rank(method=\"first\"), q=q, duplicates=\"drop\")\n        x_enc = pd.factorize(x_bin)[0]\n        scores[col] = mutual_info_score(x_enc, y_enc)\n    return pd.Series(scores).sort_values(ascending=False)\n\nmi_series = mi_scores(X_tr_bal, y_tr_bal)\n\n# --- Arama alanlarÄ± (LightGBM + k) ---\nbounds = {\n    \"learning_rate\": (0.02, 0.2),\n    \"num_leaves\": (31, 255),\n    \"max_depth\": (6, 32),\n    \"subsample\": (0.6, 1.0),\n    \"colsample_bytree\": (0.6, 1.0),\n    \"min_child_samples\": (10, 100),\n    \"reg_alpha\": (0.0, 1.0),\n    \"reg_lambda\": (0.0, 2.0),\n    \"n_estimators\": (200, 1200),\n    \"k_feats\": (20, 60)\n}\nint_keys = [\"num_leaves\", \"max_depth\", \"min_child_samples\", \"n_estimators\", \"k_feats\"]\n\ndef clamp_cast(p):\n    pp = {}\n    for k,(lo,hi) in bounds.items():\n        v = max(lo, min(hi, p[k]))\n        if k in int_keys:\n            v = int(round(v))\n        pp[k] = v\n    return pp\n\ndef sample_params():\n    p = {k: rng.uniform(lo, hi) for k,(lo,hi) in bounds.items()}\n    return clamp_cast(p)\n\n# --- Fitness fonksiyonu: birleÅŸik skor (0.5*Acc + 0.5*F1) ---\ndef fitness_score(params):\n    params = clamp_cast(params)\n    k = params.pop(\"k_feats\")\n    cols = mi_series.head(k).index\n    X = X_tr_bal[cols].values\n    y = y_tr_bal.values\n\n    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n    accs, f1s = [], []\n\n    for tr_idx, va_idx in cv.split(X, y):\n        Xtr, Xva = X[tr_idx], X[va_idx]\n        ytr, yva = y[tr_idx], y[va_idx]\n\n        clf = LGBMClassifier(\n            random_state=RANDOM_STATE,\n            class_weight=\"balanced\",\n            verbose=-1,\n            n_jobs=1,\n            **params\n        )\n\n        clf.fit(Xtr, ytr)\n        yhat = clf.predict(Xva)\n\n        accs.append(accuracy_score(yva, yhat))\n        f1s.append(f1_score(yva, yhat, average=\"macro\"))\n\n    acc = float(np.mean(accs))\n    f1m = float(np.mean(f1s))\n    return 0.5*acc + 0.5*f1m, acc, f1m, cols\n\n# --- MBO parametreleri ---\npop_size = 12\nn_gen = 8\nattract = 0.35\nflight = 0.08\nmut_p = 0.15\n\n# --- BaÅŸlat ---\npopulation = [sample_params() for _ in range(pop_size)]\nscores, details = [], []\n\nfor p in population:\n    s, a, f, cols = fitness_score(p)\n    scores.append(s)\n    details.append((a, f, cols))\n\nscores = np.array(scores)\nbest_idx = int(np.argmax(scores))\nbest_p = population[best_idx].copy()\nbest_s = float(scores[best_idx])\nbest_acc, best_f1, best_cols = details[best_idx]\n\nprint(f\"BaÅŸlangÄ±Ã§ | Score={best_s:.4f} (Acc={best_acc:.4f}, F1={best_f1:.4f}), k={len(best_cols)}\")\n\n# --- Ana dÃ¶ngÃ¼ ---\nfor g in range(n_gen):\n    new_pop = []\n    for i in range(pop_size):\n        p = population[i].copy()\n        for k,(lo,hi) in bounds.items():\n            if rng.rand() < mut_p:\n                p[k] = p[k] + rng.uniform(-0.1,0.1)*(hi-lo)\n        new_pop.append(clamp_cast(p))\n\n    for i in range(pop_size):\n        for k,(lo,hi) in bounds.items():\n            delta = best_p[k] - new_pop[i][k]\n            new_pop[i][k] = new_pop[i][k] + attract*delta + rng.uniform(-flight, flight)\n        new_pop[i] = clamp_cast(new_pop[i])\n\n    scores, details = [], []\n    for p in new_pop:\n        s, a, f, cols = fitness_score(p)\n        scores.append(s)\n        details.append((a, f, cols))\n\n    scores = np.array(scores)\n    gen_best = int(np.argmax(scores))\n    if scores[gen_best] > best_s:\n        best_s = float(scores[gen_best])\n        best_p = new_pop[gen_best].copy()\n        best_acc, best_f1, best_cols = details[gen_best]\n\n    population = new_pop\n    print(f\"Nesil {g+1}/{n_gen} | Score={best_s:.4f} (Acc={best_acc:.4f}, F1={best_f1:.4f}), k={len(best_cols)}\")\n\nprint(\"\\nâœ… MBO bitti.\")\nprint(\"En iyi parametreler:\")\nfor k,v in best_p.items():\n    print(f\" - {k}: {v}\")\n\n# --- Nihai test ---\nX_tr_final = X_tr_bal[best_cols].values\nX_te_final = X_te[best_cols].values\n\nfinal = LGBMClassifier(\n    random_state=RANDOM_STATE,\n    class_weight=\"balanced\",\n    verbose=-1,\n    n_jobs=-1,\n    **{kk:vv for kk,vv in best_p.items() if kk!='k_feats'}\n).fit(X_tr_final, y_tr_bal)\n\ny_pred = final.predict(X_te_final)\nacc = accuracy_score(y_te, y_pred)\nf1m = f1_score(y_te, y_pred, average=\"macro\")\n\nprint(f\"\\nðŸ“Š TEST (MBO+Balanced+best_k={len(best_cols)})\")\nprint(f\"Accuracy = {acc:.4f}\")\nprint(f\"Macro-F1 = {f1m:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T12:05:46.318328Z","iopub.execute_input":"2025-11-14T12:05:46.318665Z"}},"outputs":[{"name":"stdout","text":"BaÅŸlangÄ±Ã§ | Score=0.6515 (Acc=0.6544, F1=0.6486), k=42\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# === AdÄ±m 8: MBO + XGBoost hiperparametre optimizasyonu (LabelEncoder + dÃ¼zgÃ¼n hizalÄ± sÃ¼rÃ¼m) ===\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.utils import check_random_state\n\nRANDOM_STATE = 42\nrng = check_random_state(RANDOM_STATE)\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n\n# --- Etiketleri sayÄ±sallaÅŸtÄ±r (XGBoost bunu ister) ---\nlabel_enc = LabelEncoder()\ny_tr_bal_enc = label_enc.fit_transform(y_tr_bal)\ny_te_enc = label_enc.transform(y_te)\n\n# --- Parametre aralÄ±klarÄ± (XGBoost) ---\nbounds = {\n    \"learning_rate\": (0.01, 0.3),\n    \"max_depth\": (3, 15),\n    \"min_child_weight\": (1, 10),\n    \"subsample\": (0.6, 1.0),\n    \"colsample_bytree\": (0.6, 1.0),\n    \"gamma\": (0, 5),\n    \"reg_lambda\": (0.5, 3),\n    \"n_estimators\": (200, 1000),\n    \"k_feats\": (20, 60)\n}\nint_keys = [\"max_depth\", \"min_child_weight\", \"n_estimators\", \"k_feats\"]\n\ndef clamp_cast(p):\n    pp = {}\n    for k, (lo, hi) in bounds.items():\n        v = max(lo, min(hi, p[k]))\n        if k in int_keys:\n            v = int(round(v))\n        pp[k] = v\n    return pp\n\ndef sample_params():\n    p = {k: rng.uniform(lo, hi) for k, (lo, hi) in bounds.items()}\n    return clamp_cast(p)\n\n# --- Fitness fonksiyonu (Accuracy + F1) / 2 ---\ndef fitness_score(params):\n    params = clamp_cast(params)\n    k = params.pop(\"k_feats\")\n    cols = mi_series.head(k).index\n    X = X_tr_bal[cols].values\n    y = y_tr_bal_enc\n\n    accs, f1s = [], []\n    for tr_idx, va_idx in cv.split(X, y):\n        Xtr, Xva = X[tr_idx], X[va_idx]\n        ytr, yva = y[tr_idx], y[va_idx]\n\n        clf = XGBClassifier(\n            random_state=RANDOM_STATE,\n            n_jobs=1,\n            tree_method=\"hist\",\n            objective=\"multi:softmax\",\n            num_class=len(np.unique(y_tr_bal_enc)),\n            eval_metric=\"mlogloss\",\n            verbosity=0,\n            **params\n        )\n\n        clf.fit(Xtr, ytr)\n        yhat = clf.predict(Xva)\n\n        accs.append(accuracy_score(yva, yhat))\n        f1s.append(f1_score(yva, yhat, average=\"macro\"))\n\n    acc = float(np.mean(accs))\n    f1m = float(np.mean(f1s))\n    return 0.5 * acc + 0.5 * f1m, acc, f1m, cols\n\n# --- MBO ayarlarÄ± ---\npop_size = 12\nn_gen = 8\nattract = 0.35\nflight = 0.08\nmut_p = 0.15\n\n# --- PopÃ¼lasyon baÅŸlat ---\npopulation = [sample_params() for _ in range(pop_size)]\nscores, details = [], []\n\nfor p in population:\n    s, a, f, cols = fitness_score(p)\n    scores.append(s)\n    details.append((a, f, cols))\n\nscores = np.array(scores)\nbest_idx = int(np.argmax(scores))\nbest_p = population[best_idx].copy()\nbest_s = float(scores[best_idx])\nbest_acc, best_f1, best_cols = details[best_idx]\n\nprint(f\"BaÅŸlangÄ±Ã§ | Score={best_s:.4f} (Acc={best_acc:.4f}, F1={best_f1:.4f}), k={len(best_cols)}\")\n\n# --- MBO ana dÃ¶ngÃ¼sÃ¼ ---\nfor g in range(n_gen):\n    new_pop = []\n    for i in range(pop_size):\n        p = population[i].copy()\n        for k, (lo, hi) in bounds.items():\n            if rng.rand() < mut_p:\n                p[k] = p[k] + rng.uniform(-0.1, 0.1) * (hi - lo)\n        new_pop.append(clamp_cast(p))\n\n    for i in range(pop_size):\n        for k, (lo, hi) in bounds.items():\n            delta = best_p[k] - new_pop[i][k]\n            new_pop[i][k] = new_pop[i][k] + attract * delta + rng.uniform(-flight, flight)\n        new_pop[i] = clamp_cast(new_pop[i])\n\n    scores, details = [], []\n    for p in new_pop:\n        s, a, f, cols = fitness_score(p)\n        scores.append(s)\n        details.append((a, f, cols))\n\n    scores = np.array(scores)\n    gen_best = int(np.argmax(scores))\n    if scores[gen_best] > best_s:\n        best_s = float(scores[gen_best])\n        best_p = new_pop[gen_best].copy()\n        best_acc, best_f1, best_cols = details[gen_best]\n\n    population = new_pop\n    print(f\"Nesil {g+1}/{n_gen} | Score={best_s:.4f} (Acc={best_acc:.4f}, F1={best_f1:.4f}), k={len(best_cols)}\")\n\nprint(\"\\nâœ… MBO + XGBoost tamamlandÄ±.\")\nprint(\"En iyi parametreler:\")\nfor k, v in best_p.items():\n    print(f\" - {k}: {v}\")\n\n# --- Nihai test ---\nX_tr_final = X_tr_bal[best_cols].values\nX_te_final = X_te[best_cols].values\n\nfinal = XGBClassifier(\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n    tree_method=\"hist\",\n    objective=\"multi:softmax\",\n    num_class=len(np.unique(y_tr_bal_enc)),\n    eval_metric=\"mlogloss\",\n    verbosity=0,\n    **{kk: vv for kk, vv in best_p.items() if kk != \"k_feats\"}\n).fit(X_tr_final, y_tr_bal_enc)\n\ny_pred_enc = final.predict(X_te_final)\ny_pred = label_enc.inverse_transform(y_pred_enc)\n\nacc = accuracy_score(y_te, y_pred)\nf1m = f1_score(y_te, y_pred, average=\"macro\")\n\nprint(f\"\\nðŸ“Š TEST (MBO + XGBoost + best_k={len(best_cols)})\")\nprint(f\"Accuracy = {acc:.4f}\")\nprint(f\"Macro-F1 = {f1m:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"X_tr:\", X_tr.shape if 'X_tr' in globals() else None)\nprint(\"y_tr:\", len(y_tr) if 'y_tr' in globals() else None)\nprint(\"X_tr_bal:\", X_tr_bal.shape if 'X_tr_bal' in globals() else None)\nprint(\"y_tr_bal:\", len(y_tr_bal) if 'y_tr_bal' in globals() else None)\nprint(\"X_te:\", X_te.shape if 'X_te' in globals() else None)\nprint(\"y_te:\", len(y_te) if 'y_te' in globals() else None)\nprint(\"mi_series:\", \"var\" if 'mi_series' in globals() else \"yok\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === FAST MBO + XGBOOST (6x hÄ±zlandÄ±rÄ±lmÄ±ÅŸ) ===\n\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\n\nRANDOM_STATE = 42\nrng = np.random.RandomState(RANDOM_STATE)\n\nprint(\"\\nâš¡ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ MBO + XGBoost baÅŸlatÄ±lÄ±yor...\\n\")\n\n# ---------- Faster CV ----------\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n\n# ---------- XGBoost param bounds ----------\nbounds = {\n    \"learning_rate\": (0.01, 0.3),\n    \"max_depth\": (3, 12),\n    \"min_child_weight\": (1, 8),\n    \"subsample\": (0.6, 1.0),\n    \"colsample_bytree\": (0.6, 1.0),\n    \"gamma\": (0, 3),\n    \"reg_lambda\": (0.3, 2),\n    \"n_estimators\": (200, 400),    # 1200 â†’ 400 (Ã§ok hÄ±zlanÄ±r)\n    \"k_feats\": (15, 40)            # 20â€“60 â†’ 15â€“40\n}\nint_keys = [\"max_depth\", \"min_child_weight\", \"n_estimators\", \"k_feats\"]\n\ndef clamp(p):\n    out = {}\n    for k, (lo, hi) in bounds.items():\n        v = max(lo, min(hi, p[k]))\n        if k in int_keys:\n            v = int(round(v))\n        out[k] = v\n    return out\n\n\ndef random_params():\n    return clamp({k: rng.uniform(lo, hi) for k, (lo, hi) in bounds.items()})\n\n\n# -------- Fitness --------\ndef fitness(params):\n    p = clamp(params)\n    k = p[\"k_feats\"]\n    cols = mi_series_ros.head(k).index.tolist()\n\n    Xsel = X_ros[:, cols]\n    ysel = y_ros\n\n    accs, f1s = [], []\n\n    for tr_idx, va_idx in cv.split(Xsel, ysel):\n        Xtr, Xva = Xsel[tr_idx], Xsel[va_idx]\n        ytr, yva = ysel[tr_idx], ysel[va_idx]\n\n        model = XGBClassifier(\n            random_state=42,\n            n_jobs=1,\n            tree_method=\"approx\",    # âš¡ Ã§ok hÄ±zlÄ±\n            eval_metric=\"mlogloss\",\n            objective=\"multi:softmax\",\n            num_class=len(np.unique(ysel)),\n            **{k: v for k, v in p.items() if k != \"k_feats\"}\n        )\n\n        model.fit(Xtr, ytr)\n        pred = model.predict(Xva)\n\n        accs.append(accuracy_score(yva, pred))\n        f1s.append(f1_score(yva, pred, average=\"macro\"))\n\n    score = 0.5*(np.mean(accs) + np.mean(f1s))\n    return score, np.mean(accs), np.mean(f1s), cols\n\n\n# -------- MBO Loop --------\npop_size = 8     # 12 â†’ 8 (yine hÄ±z)\ngens = 5         # 8 â†’ 5 (yine hÄ±z)\n\npopulation = [random_params() for _ in range(pop_size)]\nbest_score, best_p, best_cols = -1, None, None\n\nprint(\"BaÅŸlangÄ±Ã§ nÃ¼fusu deÄŸerlendiriliyor...\\n\")\n\nfor p in population:\n    s, a, f, c = fitness(p)\n    if s > best_score:\n        best_score, best_p, best_cols = s, p, c\n\nprint(f\"ðŸš€ BaÅŸlangÄ±Ã§ Score={best_score:.4f}\")\n\n\nfor g in range(gens):\n    new_pop = []\n\n    for p in population:\n        q = p.copy()\n        for k, (lo, hi) in bounds.items():\n            if rng.rand() < 0.15:\n                q[k] += rng.uniform(-0.1, 0.1)*(hi-lo)\n        new_pop.append(clamp(q))\n\n    for i in range(pop_size):\n        for k in bounds.keys():\n            new_pop[i][k] += 0.3*(best_p[k] - new_pop[i][k]) + rng.uniform(-0.05, 0.05)\n        new_pop[i] = clamp(new_pop[i])\n\n    for p in new_pop:\n        s, a, f, c = fitness(p)\n        if s > best_score:\n            best_score, best_p, best_cols = s, p, c\n\n    population = new_pop\n    print(f\"Nesil {g+1}/{gens} | Score={best_score:.4f}\")\n\n\n# ---------- Final Test ----------\nprint(\"\\nðŸ“Œ Nihai test hesaplanÄ±yor...\\n\")\n\nX_te_final = X_te.values[:, best_cols]\n\nfinal_model = XGBClassifier(\n    random_state=42,\n    n_jobs=-1,\n    tree_method=\"approx\",\n    eval_metric=\"mlogloss\",\n    objective=\"multi:softmax\",\n    num_class=len(np.unique(y_ros)),\n    **{k: v for k, v in best_p.items() if k != \"k_feats\"}\n)\n\nfinal_model.fit(X_ros[:, best_cols], y_ros)\n\npred_enc = final_model.predict(X_te_final)\npred = label_enc.inverse_transform(pred_enc)\n\nacc = accuracy_score(y_te, pred)\nf1m = f1_score(y_te, pred, average=\"macro\")\n\nprint(\"ðŸ“Š TEST (FAST MBO + XGBoost)\")\nprint(\"Accuracy:\", acc)\nprint(\"F1-macro:\", f1m)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === ADIM 9: imblearn YOK â†’ Manuel Random Oversampling + MBO + XGBoost ===\nprint(\"ðŸ“Œ AÅŸama baÅŸlÄ±yor: Manuel Oversampling + MBO + XGBoost\\n\")\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom xgboost import XGBClassifier\n\nRANDOM_STATE = 42\nrng = np.random.RandomState(RANDOM_STATE)\n\n# --- 1) Etiket Encode ---\nlabel_enc = LabelEncoder()\ny_tr_enc = label_enc.fit_transform(y_tr_bal)   # âœ” dÃ¼zeltildi\ny_te_enc = label_enc.transform(y_te)\n\n# --- 2) Manuel Oversampling (imblearn yok) ---\nprint(\"ðŸ“Œ Manuel Oversampling uygulanÄ±yor...\")\n\nX_arr = X_tr.values\ny_arr = y_tr_enc\n\nunique, counts = np.unique(y_arr, return_counts=True)\nmax_count = counts.max()\n\nX_os = []\ny_os = []\n\nfor cls in unique:\n    cls_idx = np.where(y_arr == cls)[0]\n    needed = max_count - len(cls_idx)\n    \n    # mevcutlarÄ± ekle\n    X_os.append(X_arr[cls_idx])\n    y_os.append(y_arr[cls_idx])\n    \n    # eksik olan kadar rastgele tekrar ekle\n    extra_idx = rng.choice(cls_idx, size=needed, replace=True)\n    X_os.append(X_arr[extra_idx])\n    y_os.append(y_arr[extra_idx])\n\nX_ros = np.vstack(X_os)\ny_ros = np.hstack(y_os)\n\nprint(\"Oversampling sonrasÄ± boyut:\", X_ros.shape)\n\n\n# --- 3) MI yeniden hesapla ---\nprint(\"\\nðŸ“Œ Bilgi kazancÄ± (MI) hesaplanÄ±yor...\")\n\nfrom sklearn.metrics import mutual_info_score\n\nmi_scores = {}\nfor i in range(X_ros.shape[1]):\n    x_bin = pd.qcut(X_ros[:, i], q=10, duplicates=\"drop\")\n    x_enc = pd.factorize(x_bin)[0]\n    mi_scores[i] = mutual_info_score(x_enc, y_ros)\n\nmi_series_ros = pd.Series(mi_scores).sort_values(ascending=False)\nprint(\"En yÃ¼ksek MI Ã¶zellikleri:\", mi_series_ros.head(10).index.tolist())\n\n\n# --- 4) MBO Parametre AlanlarÄ± ---\nbounds = {\n    \"learning_rate\": (0.01, 0.3),\n    \"max_depth\": (3, 15),\n    \"min_child_weight\": (1, 10),\n    \"subsample\": (0.6, 1.0),\n    \"colsample_bytree\": (0.6, 1.0),\n    \"gamma\": (0, 5),\n    \"reg_lambda\": (0.5, 3),\n    \"n_estimators\": (200, 1000),\n    \"k_feats\": (20, 60),\n}\nint_keys = [\"max_depth\", \"min_child_weight\", \"n_estimators\", \"k_feats\"]\n\ndef clamp(p):\n    out = {}\n    for k, (low, high) in bounds.items():\n        v = max(low, min(high, p[k]))\n        if k in int_keys:\n            v = int(round(v))\n        out[k] = v\n    return out\n\ndef random_params():\n    return clamp({k: rng.uniform(lo, hi) for k, (lo, hi) in bounds.items()})\n\n\n# --- 5) Fitness Fonksiyonu ---\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\ndef fitness(params):\n    params = clamp(params)\n    k = params[\"k_feats\"]\n    cols = mi_series_ros.head(k).index.tolist()\n    \n    X_sel = X_ros[:, cols]\n    y_sel = y_ros\n\n    accs = []\n    f1s  = []\n\n    for tr_idx, va_idx in cv.split(X_sel, y_sel):\n        Xtr, Xva = X_sel[tr_idx], X_sel[va_idx]\n        ytr, yva = y_sel[tr_idx], y_sel[va_idx]\n\n        clf = XGBClassifier(\n            random_state=42,\n            n_jobs=1,\n            tree_method=\"hist\",\n            objective=\"multi:softmax\",\n            eval_metric=\"mlogloss\",\n            num_class=len(np.unique(y_sel)),\n            **{kk: vv for kk, vv in params.items() if kk != \"k_feats\"}\n        )\n\n        clf.fit(Xtr, ytr)\n        yhat = clf.predict(Xva)\n\n        accs.append(accuracy_score(yva, yhat))\n        f1s.append(f1_score(yva, yhat, average=\"macro\"))\n\n    acc = np.mean(accs)\n    f1m = np.mean(f1s)\n    return 0.5 * acc + 0.5 * f1m, acc, f1m, cols\n\n\n# --- 6) MBO BaÅŸlat ---\npop_size = 12\ngens = 8\n\npopulation = [random_params() for _ in range(pop_size)]\nbest_score = -1\nbest_params = None\nbest_cols = None\n\nprint(\"\\nðŸ“Œ MBO baÅŸlatÄ±lÄ±yor...\\n\")\n\n# baÅŸlangÄ±Ã§\nfor p in population:\n    s,a,f,c = fitness(p)\n    if s > best_score:\n        best_score, best_params, best_cols = s,p,c\n\nprint(f\"BaÅŸlangÄ±Ã§ Score={best_score:.4f}\")\n\n\n# --- 7) MBO Ana DÃ¶ngÃ¼ ---\nfor g in range(gens):\n    new_pop = []\n    for p in population:\n        q = p.copy()\n        for k,(lo,hi) in bounds.items():\n            if rng.rand() < 0.15:\n                q[k] += rng.uniform(-0.1, 0.1)*(hi-lo)\n        new_pop.append(clamp(q))\n\n    for i in range(pop_size):\n        for k in bounds.keys():\n            new_pop[i][k] += 0.35*(best_params[k] - new_pop[i][k]) + rng.uniform(-0.08, 0.08)\n        new_pop[i] = clamp(new_pop[i])\n\n    for p in new_pop:\n        s,a,f,c = fitness(p)\n        if s > best_score:\n            best_score, best_params, best_cols = s,p,c\n\n    population = new_pop\n    print(f\"Nesil {g+1}/{gens} | Score={best_score:.4f}\")\n\n\n# --- 8) Nihai Test ---\nprint(\"\\nðŸ“Œ Nihai test yapÄ±lÄ±yor...\\n\")\n\nX_te_final = X_te.values[:, best_cols]\n\nfinal = XGBoost_final = XGBClassifier(\n    random_state=42,\n    n_jobs=-1,\n    tree_method=\"hist\",\n    objective=\"multi:softmax\",\n    eval_metric=\"mlogloss\",\n    num_class=len(np.unique(y_ros)),\n    **{kk: vv for kk, vv in best_params.items() if kk!=\"k_feats\"}\n)\n\nfinal.fit(X_ros[:, best_cols], y_ros)\ny_pred_enc = final.predict(X_te_final)\ny_pred = label_enc.inverse_transform(y_pred_enc)\n\nacc = accuracy_score(y_te, y_pred)\nf1m = f1_score(y_te, y_pred, average=\"macro\")\n\nprint(\"ðŸ“Š TEST Sonucu (Oversampling + MBO + XGBoost)\")\nprint(\"Accuracy:\", acc)\nprint(\"F1-macro:\", f1m)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}