{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4059918,"sourceType":"datasetVersion","datasetId":2398189}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === ADIM 1: CIC-DDoS 2019 dataset birleÅŸtirme ve temizleme ===\n\nimport os, glob, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nDATA_DIR = \"/kaggle/input/cicddos2019\"\nTARGET   = \"Label\"\n\ndef extract_label_from_filename(fname):\n    \"\"\"\n    Ã–rn: 'Syn-training.parquet' â†’ 'Syn'\n    \"\"\"\n    return os.path.basename(fname).split(\"-\")[0]\n\n# 1) TÃ¼m parquet dosyalarÄ±nÄ± yÃ¼kle\nframes = []\nfor path in sorted(glob.glob(os.path.join(DATA_DIR, \"*.parquet\"))):\n    df = pd.read_parquet(path)\n    \n    # Etiket ekle\n    df[TARGET] = extract_label_from_filename(path)\n    \n    frames.append(df)\n\n# 2) TÃ¼m veriyi tek dataframeâ€™de birleÅŸtir\nfull_df = pd.concat(frames, ignore_index=True)\nprint(\"Ham veri boyutu:\", full_df.shape)\n\n# 3) Inf / NaN temizle\nfull_df = full_df.replace([np.inf, -np.inf], np.nan)\nfull_df = full_df.dropna(axis=0)   # Ã§ok bÃ¼yÃ¼k dataset olduÄŸundan satÄ±r silmek gÃ¼venli\nprint(\"NaN temizliÄŸi sonrasÄ±:\", full_df.shape)\n\n# 4) Gereksiz kolonlar (Flow ID, Timestamp, vs.)\ndrop_cols = [c for c in full_df.columns \n             if \"Flow ID\" in c or \"Timestamp\" in c or \"Src IP\" in c or \"Dst IP\" in c]\n\nif drop_cols:\n    full_df = full_df.drop(columns=drop_cols)\n    print(\"Gereksiz ID/IP kolonlarÄ± silindi:\", drop_cols)\n\n# 5) Numeric olmayan kolonlarÄ± at\nnumeric_cols = [c for c in full_df.columns if pd.api.types.is_numeric_dtype(full_df[c])]\nnon_numeric  = [c for c in full_df.columns if c not in numeric_cols + [TARGET]]\n\nif non_numeric:\n    full_df = full_df.drop(columns=non_numeric)\n    print(\"SayÄ±sal olmayan kolonlar silindi:\", non_numeric)\n\n# 6) Sabit (nunique=1) kolonlarÄ± at\nnunique = full_df[numeric_cols].nunique()\nconst_cols = nunique[nunique <= 1].index.tolist()\n\nif const_cols:\n    full_df = full_df.drop(columns=const_cols)\n    print(\"Sabit kolonlar silindi:\", const_cols)\n\nprint(\"\\nğŸ“Œ ADIM 1 TAMAMLANDI â€” TEMÄ°Z DATA\")\nprint(\"Final boyut:\", full_df.shape)\nprint(\"SÄ±nÄ±flar:\", full_df[TARGET].unique())\n","metadata":{"_uuid":"d97dd841-3ce0-45da-bc08-5f017a275a1f","_cell_guid":"dd49f0e8-16c3-4105-b98d-b45bb42a3c7d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-19T09:17:00.392365Z","iopub.execute_input":"2025-11-19T09:17:00.394037Z","iopub.status.idle":"2025-11-19T09:17:02.867270Z","shell.execute_reply.started":"2025-11-19T09:17:00.393992Z","shell.execute_reply":"2025-11-19T09:17:02.866033Z"}},"outputs":[{"name":"stdout","text":"Ham veri boyutu: (431371, 78)\nNaN temizliÄŸi sonrasÄ±: (431371, 78)\nSabit kolonlar silindi: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'FIN Flag Count', 'PSH Flag Count', 'ECE Flag Count', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\n\nğŸ“Œ ADIM 1 TAMAMLANDI â€” TEMÄ°Z DATA\nFinal boyut: (431371, 66)\nSÄ±nÄ±flar: ['DNS' 'LDAP' 'MSSQL' 'NTP' 'NetBIOS' 'Portmap' 'SNMP' 'Syn' 'TFTP' 'UDP'\n 'UDPLag']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"TÃ¼m CIC-DDoS-2019 trafik dosyalarÄ± birleÅŸik bir veri setinde toplandÄ± ve etiketler dosya adÄ±ndan otomatik olarak Ã¼retildi. Flow ID, Timestamp ve IP adresleri gibi saldÄ±rÄ± tipiyle iliÅŸkisi olmayan kolonlar Ã§Ä±karÄ±ldÄ±. Inf/NaN deÄŸerler temizlendi, sabit ve sayÄ±sal olmayan deÄŸiÅŸkenler veri setinden atÄ±ldÄ±. Bu iÅŸlem sonucunda tamamen temizlenmiÅŸ ve analiz iÃ§in hazÄ±r bir veri seti elde edilmiÅŸtir.","metadata":{}},{"cell_type":"code","source":"# === ADIM 2: Stratified Train/Test Split (%80 / %20) ===\n\nfrom sklearn.model_selection import train_test_split\n\ndf_clean = full_df.copy()   # AdÄ±m 1'den gelen temiz dataset\n\nX = df_clean.drop(columns=[TARGET])\ny = df_clean[TARGET]\n\nprint(\"TÃ¼m veri:\", X.shape)\n\n# Stratified split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    random_state=42,\n    shuffle=True,\n    stratify=y\n)\n\nprint(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\nprint(\"\\nTrain sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:\\n\", y_train.value_counts())\nprint(\"\\nTest sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:\\n\", y_test.value_counts())\n","metadata":{"_uuid":"0f7a5f92-3786-41b1-8aec-878d30912896","_cell_guid":"ff4dadba-861d-4435-8497-1bacc23d4d7d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-19T09:17:02.869320Z","iopub.execute_input":"2025-11-19T09:17:02.869705Z","iopub.status.idle":"2025-11-19T09:17:03.473859Z","shell.execute_reply.started":"2025-11-19T09:17:02.869679Z","shell.execute_reply":"2025-11-19T09:17:03.472565Z"}},"outputs":[{"name":"stdout","text":"TÃ¼m veri: (431371, 65)\nTrain: (345096, 65) | Test: (86275, 65)\n\nTrain sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:\n Label\nNTP        107739\nTFTP        97466\nSyn         56994\nUDP         24186\nUDPLag      20083\nMSSQL       15246\nLDAP         7637\nDNS          5362\nPortmap      4084\nSNMP         3214\nNetBIOS      3085\nName: count, dtype: int64\n\nTest sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:\n Label\nNTP        26935\nTFTP       24367\nSyn        14249\nUDP         6046\nUDPLag      5021\nMSSQL       3811\nLDAP        1909\nDNS         1341\nPortmap     1021\nSNMP         804\nNetBIOS      771\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"TÃ¼m veri kÃ¼mesi rastgele karÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ ve sÄ±nÄ±f oranlarÄ±nÄ±n korunmasÄ± amacÄ±yla Stratified Train-Test Split yÃ¶ntemiyle %80 eÄŸitim ve %20 test olacak ÅŸekilde bÃ¶lÃ¼nmÃ¼ÅŸtÃ¼r. CIC-DDoS-2019 veri seti farklÄ± gÃ¼nlerde toplanmÄ±ÅŸ trafiklerden oluÅŸtuÄŸu iÃ§in orijinal dosya bazlÄ± bÃ¶lme yaklaÅŸÄ±mÄ± daÄŸÄ±lÄ±m tutarsÄ±zlÄ±ÄŸÄ± yaratmaktadÄ±r. Bu nedenle stratified split kullanÄ±mÄ± sÄ±nÄ±flandÄ±rma algoritmalarÄ±nÄ±n optimizasyon etkisini daha doÄŸru Ã¶lÃ§mek iÃ§in tercih edilmiÅŸtir.","metadata":{}},{"cell_type":"code","source":"# === ADIM 3: Custom SMOTE (imblearn olmadan) ===\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport pandas as pd\n\n# 1) Label Encode\nenc = LabelEncoder()\ny_train_enc = enc.fit_transform(y_train)\ny_test_enc  = enc.transform(y_test)\n\nprint(\"SÄ±nÄ±f -> ID eÅŸleÅŸmesi:\", dict(zip(enc.classes_, enc.transform(enc.classes_))))\n\n# 2) Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled  = scaler.transform(X_test)\n\n# 3) Custom SMOTE fonksiyonu\ndef custom_smote(X, y, k=5):\n    X = np.array(X)\n    y = np.array(y)\n\n    classes, counts = np.unique(y, return_counts=True)\n    max_count = counts.max()\n\n    new_X = [X]\n    new_y = [y]\n\n    for cls, count in zip(classes, counts):\n        if count == max_count:\n            continue\n\n        need = max_count - count\n        cls_idx = np.where(y == cls)[0]\n        X_cls = X[cls_idx]\n\n        nn = NearestNeighbors(n_neighbors=min(k, len(X_cls)))\n        nn.fit(X_cls)\n        neigh_idx = nn.kneighbors(X_cls, return_distance=False)\n\n        synthetic = []\n        for i in range(need):\n            row = np.random.choice(cls_idx)\n            neigh = np.random.choice(neigh_idx[row % len(X_cls)])\n            diff = X[neigh] - X[row]\n            synthetic.append(X[row] + np.random.rand() * diff)\n\n        new_X.append(np.array(synthetic))\n        new_y.append(np.full(need, cls))\n\n    X_out = np.vstack(new_X)\n    y_out = np.concatenate(new_y)\n    return X_out, y_out\n\n# 4) SMOTE uygula\nX_train_bal, y_train_bal = custom_smote(X_train_scaled, y_train_enc, k=5)\n\nprint(\"\\nYeni Train boyutu:\", X_train_bal.shape)\nprint(\"Yeni daÄŸÄ±lÄ±m:\", pd.Series(y_train_bal).value_counts())\n","metadata":{"_uuid":"612c2853-0795-4a12-853b-c60c5716186d","_cell_guid":"fad78e7f-59a7-47ab-b995-f4b8c06fbd73","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-19T09:17:03.474897Z","iopub.execute_input":"2025-11-19T09:17:03.475209Z","iopub.status.idle":"2025-11-19T09:18:36.701934Z","shell.execute_reply.started":"2025-11-19T09:17:03.475186Z","shell.execute_reply":"2025-11-19T09:18:36.701000Z"}},"outputs":[{"name":"stdout","text":"SÄ±nÄ±f -> ID eÅŸleÅŸmesi: {'DNS': 0, 'LDAP': 1, 'MSSQL': 2, 'NTP': 3, 'NetBIOS': 4, 'Portmap': 5, 'SNMP': 6, 'Syn': 7, 'TFTP': 8, 'UDP': 9, 'UDPLag': 10}\n\nYeni Train boyutu: (1185129, 65)\nYeni daÄŸÄ±lÄ±m: 3     107739\n8     107739\n2     107739\n10    107739\n9     107739\n7     107739\n1     107739\n4     107739\n5     107739\n6     107739\n0     107739\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Etiketler, sÄ±nÄ±flandÄ±rma algoritmalarÄ±nda kullanÄ±lmak Ã¼zere tamsayÄ± deÄŸerler ile kodlanmÄ±ÅŸtÄ±r (Label Encoding). Bu, Ã§ok sÄ±nÄ±flÄ± optimizasyon sÃ¼recini kolaylaÅŸtÄ±rmak iÃ§in standart bir adÄ±mdÄ±r.SayÄ±sal Ã¶zellikler, standart sapma ve ortalama bazlÄ± olarak Ã¶lÃ§eklendirilmiÅŸ (StandardScaler) ve tÃ¼m Ã¶zellikler aynÄ± bÃ¼yÃ¼klÃ¼k mertebesine getirilmiÅŸtir. Bu, hem sÄ±nÄ±flandÄ±rma algoritmalarÄ±nÄ±n kararlÄ±lÄ±ÄŸÄ±nÄ± hem de meta-sezgisel optimizasyonun performansÄ±nÄ± artÄ±rÄ±r.","metadata":{}},{"cell_type":"code","source":"print(\"X_train_bal var mÄ±?\", 'X_train_bal' in globals())\nprint(\"y_train_bal var mÄ±?\", 'y_train_bal' in globals())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T09:18:36.702754Z","iopub.execute_input":"2025-11-19T09:18:36.703121Z","iopub.status.idle":"2025-11-19T09:18:36.709588Z","shell.execute_reply.started":"2025-11-19T09:18:36.703016Z","shell.execute_reply":"2025-11-19T09:18:36.708397Z"}},"outputs":[{"name":"stdout","text":"X_train_bal var mÄ±? True\ny_train_bal var mÄ±? True\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# === ADIM 4 (Optimize): Baseline modellerin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± ===\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nimport pandas as pd\nimport numpy as np\n\nRANDOM_STATE = 42\n\nprint(\"Train (balanced):\", X_train_bal.shape)\nprint(\"Test (scaled):   \", X_test_scaled.shape)\n\n# ğŸ”¥ SADELEÅTÄ°RÄ°LMÄ°Å MODELLER â€” HEPSÄ° VERÄ°YE UYGUN VE HIZLI\nmodels = {\n    \"NaiveBayes\": GaussianNB(),\n\n    \"RandomForest\": RandomForestClassifier(\n        n_estimators=200,\n        random_state=RANDOM_STATE,\n        n_jobs=-1\n    ),\n\n    \"LightGBM\": LGBMClassifier(\n        random_state=RANDOM_STATE,\n        n_estimators=200,\n        n_jobs=-1\n    ),\n\n    \"XGBoost\": XGBClassifier(\n        random_state=RANDOM_STATE,\n        n_estimators=200,\n        n_jobs=-1,\n        tree_method=\"hist\",\n        objective=\"multi:softmax\",\n        num_class=len(np.unique(y_train_bal)),\n        eval_metric=\"mlogloss\",\n        verbosity=0,\n    ),\n}\n\n# 3-fold CV\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n\nbaseline_results = []\n\nprint(\"\\nâœ… Baz modeller (3-fold CV, balanced data):\\n\")\nfor name, clf in models.items():\n    acc_scores = cross_val_score(\n        clf, X_train_bal, y_train_bal,\n        cv=cv, scoring=\"accuracy\"\n    )\n    f1_scores = cross_val_score(\n        clf, X_train_bal, y_train_bal,\n        cv=cv, scoring=\"f1_macro\"\n    )\n\n    acc_mean = acc_scores.mean()\n    f1_mean  = f1_scores.mean()\n\n    baseline_results.append({\n        \"Model\": name,\n        \"CV_Accuracy\": acc_mean,\n        \"CV_MacroF1\": f1_mean\n    })\n\n    print(f\"{name:10s} | Acc={acc_mean:.4f} | F1-macro={f1_mean:.4f}\")\n\nbaseline_df = pd.DataFrame(baseline_results).sort_values(\n    \"CV_Accuracy\", ascending=False\n).reset_index(drop=True)\n\nprint(\"\\nğŸ“Š Baz modellerin CV sonuÃ§ Ã¶zeti:\")\nprint(baseline_df)\n\n# Test aÅŸamasÄ± â€” ilk 2 modeli testte deÄŸerlendirelim\ntop_models = baseline_df[\"Model\"].head(2).tolist()\n\nprint(\"\\nğŸ§ª Test seti deÄŸerlendirmesi (ilk 2 model):\\n\")\n\nfor name in top_models:\n    clf = models[name]\n    clf.fit(X_train_bal, y_train_bal)\n    y_pred_test = clf.predict(X_test_scaled)\n\n    acc_test = accuracy_score(y_test_enc, y_pred_test)\n    f1_test  = f1_score(y_test_enc, y_pred_test, average=\"macro\")\n\n    print(f\"=== {name} ===\")\n    print(f\"Test Accuracy : {acc_test:.4f}\")\n    print(f\"Test Macro-F1 : {f1_test:.4f}\")\n    print(\"SÄ±nÄ±f bazlÄ± rapor:\")\n    print(classification_report(y_test_enc, y_pred_test, digits=4))\n    print(\"-\" * 60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bu adÄ±mda, SMOTE ile dengelenmiÅŸ eÄŸitim verisi kullanÄ±larak dÃ¶rt farklÄ± sÄ±nÄ±flandÄ±rma algoritmasÄ± (Naive Bayes, Random Forest, LightGBM ve XGBoost) 3 katlÄ± Ã§apraz doÄŸrulama ile deÄŸerlendirilmiÅŸtir. AmaÃ§, optimizasyon uygulanmadan Ã¶nce modellerin temel (baseline) doÄŸruluk ve Macro-F1 performanslarÄ±nÄ± belirlemek ve ilerleyen adÄ±mlarda meta-sezgisel optimizasyon teknikleri ile elde edilecek iyileÅŸmeyi karÅŸÄ±laÅŸtÄ±rmak iÃ§in referans deÄŸerler oluÅŸturmaktÄ±r. Ã‡apraz doÄŸrulamanÄ±n ardÄ±ndan en yÃ¼ksek performansÄ± gÃ¶steren ilk iki model gerÃ§ek test kÃ¼mesi Ã¼zerinde ayrÄ±ca deÄŸerlendirilmiÅŸtir.","metadata":{}}]}